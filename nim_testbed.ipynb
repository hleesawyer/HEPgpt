{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "import subprocess\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_line_cell_magic\n",
    "def nim(line, cell):\n",
    "    fp = open(\"for_nim_magic.nim\", \"w\")\n",
    "    fp.write(cell)\n",
    "    fp.close()\n",
    "    p = subprocess.Popen([\"nim\", \"compile\", \"--run\", \"for_nim_magic.nim\"], stdout=subprocess.PIPE)\n",
    "    out, err = p.communicate()\n",
    "    if err:\n",
    "        print(err.decode(\"utf-8\"), file=sys.stderr)\n",
    "    if out:\n",
    "        print(out.decode(\"utf-8\"),\"\\n\\n\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF THE URLS need \"-d:ssl\"\n",
    "\n",
    "@register_line_cell_magic\n",
    "def nim(line, cell):\n",
    "    fp = open(\"for_nim_magic.nim\", \"w\")\n",
    "    fp.write(cell)\n",
    "    fp.close()\n",
    "    p = subprocess.Popen([\"nim\", \"compile\", \"--run\", \"-d:ssl\", \"for_nim_magic.nim\"], stdout=subprocess.PIPE)\n",
    "    out, err = p.communicate()\n",
    "    if err:\n",
    "        print(err.decode(\"utf-8\"), file=sys.stderr)\n",
    "    if out:\n",
    "        print(out.decode(\"utf-8\"),\"\\n\\n\\n\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT!\n",
    "In the robots.txt file of \"https://atlassoftwaredocs.web.cern.ch\" it says \"Sitemap: /sitemap.xml\"\n",
    "\n",
    "Thus, we can navigate to \"https://atlassoftwaredocs.web.cern.ch/sitemap.xml\" to simply get all the urls \n",
    "that will populate our robots.txt file according to the documentation on this domain!\n",
    "\n",
    "##### Other things to Do\n",
    "- anti crawler blocking tactics: https://oxylabs.io/blog/how-to-crawl-a-website-without-getting-blocked\n",
    "- better/more documentation data from cern (talk to sawyer)\n",
    "\n",
    "##### A Problem:\n",
    "https://project-search.web.cern.ch/ <br>\n",
    "There appears to be a cern search project underway that will simplify the online documentation for cern. It directly conflicts with our project, but it looks like they havn't done anything yet, so we need to move very fast.\n",
    "\n",
    "##### EVEN MORE and BETTER\n",
    "search.cern.ch ...a crawyer that catalogs and makes accessible all the cern documents! WE CAN SCRAP THIS FOR ALMOST EVERYTHING AND MAKE THE SYsTEM!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATLAS Robots.txt file ...the \"/\" in front of \"#\" are not part of normal robots.txt file -- see nim-robotparser <br><br>\n",
    "/#\n",
    "/# robots.txt\n",
    "/#\n",
    "/# This file is to prevent the crawling and indexing of certain parts\n",
    "/# of your site by web crawlers and spiders run by sites like Yahoo!\n",
    "/# and Google. By telling these \"robots\" where not to go on your site,\n",
    "/# you save bandwidth and server resources.\n",
    "/#\n",
    "/# This file will be ignored unless it is at the root of your host:\n",
    "/# Used:    http://example.com/robots.txt\n",
    "/# Ignored: http://example.com/site/robots.txt\n",
    "/#\n",
    "/# For more information about the robots.txt standard, see:\n",
    "/# http://www.robotstxt.org/robotstxt.html\n",
    "\n",
    "User-agent: *\n",
    "/# CSS, JS, Images\n",
    "Allow: /core/*.css$\n",
    "Allow: /core/*.css?\n",
    "Allow: /core/*.js$\n",
    "Allow: /core/*.js?\n",
    "Allow: /core/*.gif\n",
    "Allow: /core/*.jpg\n",
    "Allow: /core/*.jpeg\n",
    "Allow: /core/*.png\n",
    "Allow: /core/*.svg\n",
    "Allow: /profiles/*.css$\n",
    "Allow: /profiles/*.css?\n",
    "Allow: /profiles/*.js$\n",
    "Allow: /profiles/*.js?\n",
    "Allow: /profiles/*.gif\n",
    "Allow: /profiles/*.jpg\n",
    "Allow: /profiles/*.jpeg\n",
    "Allow: /profiles/*.png\n",
    "Allow: /profiles/*.svg\n",
    "/# Directories\n",
    "Disallow: /core/\n",
    "Disallow: /profiles/\n",
    "/# Files\n",
    "Disallow: /README.txt\n",
    "Disallow: /web.config\n",
    "/# Paths (clean URLs)\n",
    "Disallow: /admin/\n",
    "Disallow: /comment/reply/\n",
    "Disallow: /filter/tips\n",
    "Disallow: /node/add/\n",
    "Disallow: /search/\n",
    "Disallow: /user/register\n",
    "Disallow: /user/password\n",
    "Disallow: /user/login\n",
    "Disallow: /user/logout\n",
    "Disallow: /media/oembed\n",
    "Disallow: /*/media/oembed\n",
    "/# Paths (no clean URLs)\n",
    "Disallow: /index.php/admin/\n",
    "Disallow: /index.php/comment/reply/\n",
    "Disallow: /index.php/filter/tips\n",
    "Disallow: /index.php/node/add/\n",
    "Disallow: /index.php/search/\n",
    "Disallow: /index.php/user/password\n",
    "Disallow: /index.php/user/register\n",
    "Disallow: /index.php/user/login\n",
    "Disallow: /index.php/user/logout\n",
    "Disallow: /index.php/media/oembed\n",
    "Disallow: /index.php/*/media/oembed\n",
    "Crawl-Delay: 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hint: used config file '/home/wrkn/miniconda3/envs/hepgpt/nim/config/nim.cfg' [Conf]\n",
      "Hint: used config file '/home/wrkn/miniconda3/envs/hepgpt/nim/config/config.nims' [Conf]\n",
      "...............................................\n",
      "Hint:  [Link]\n",
      "Hint: 99351 lines; 3.611s; 116.348MiB peakmem; Debug build; proj: /home/wrkn/GitRepos/HEPgpt/for_nim_magic.nim; out: /home/wrkn/GitRepos/HEPgpt/for_nim_magic [SuccessX]\n",
      "Hint: /home/wrkn/GitRepos/HEPgpt/for_nim_magic  [Exec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC: stdlib_io.nim\n",
      "CC: stdlib_system.nim\n",
      "CC: for_nim_magic.nim\n",
      "\n",
      "Request 0 completed.\n",
      "\n",
      "randDelay=20451ms\n",
      "Request 1 completed.\n",
      "\n",
      "randDelay=25098ms\n",
      "All procesing complete.\n",
      " \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%nim\n",
    "import std/strformat\n",
    "import std/httpclient\n",
    "import strutils\n",
    "import std/os\n",
    "import std/random\n",
    "\n",
    "# import std/rdstdin\n",
    "\n",
    "# TODO\n",
    "# - process the html\n",
    "# - get all the data\n",
    "\n",
    "\n",
    "### FOR ASYNC REQUESTS USE ###\n",
    "# Async is used when program speed is I/O bound rather than cpu bound\n",
    "# import std/[asyncdispatch, httpclient]\n",
    "\n",
    "# proc asyncProc(): Future[string] {.async.} =\n",
    "#   var client = newAsyncHttpClient()\n",
    "#   return await client.getContent(\"http://example.com\")\n",
    "\n",
    "# echo waitFor asyncProc()\n",
    "\n",
    "##############################\n",
    "\n",
    "proc return_request*(url: string, idx: int) = \n",
    "    \n",
    "    # runnableExamples:\n",
    "        # my_request = \"http://export.arxiv.org/api/query?search_query=all:electron&start=0&max_results=10\"\n",
    "        # echo return_request(my_request)\n",
    "\n",
    "    var \n",
    "        client = newHttpClient()\n",
    "        # Add the request here in this line\n",
    "        d = client.request(url)\n",
    "        data = d.body()\n",
    "\n",
    "    writeFile(&\"/home/wrkn/GitRepos/HEPgpt/data-gathering/data/{url.replace('/','.')}_unparsed_{idx}.txt\",data)\n",
    "\n",
    "\n",
    "    # PROCESS IT LATER,htmlparser? ######################3\n",
    "    # Line by line, we read the file by\n",
    "    # let f = open(&\"request_results_{idx}.txt\")\n",
    "\n",
    "    # var line: string\n",
    "    # try:\n",
    "    #     var in_line: bool = false\n",
    "    #     # while not endOfFile(f):\n",
    "    #         # line = f.readLine()\n",
    "    #     for line in lines(f):\n",
    "\n",
    "    #         if \"<title>\" in line:\n",
    "    #             in_line = true\n",
    "    #             echo line.replace(\"<title>\",\"Title:\")\n",
    "    #         if \"<summary>\" in line:\n",
    "    #             in_line = true\n",
    "    #             echo line.replace(\"<summary>\",\"Summary:\")\n",
    "\n",
    "    #         while in_line:\n",
    "    #             line = f.readLine()\n",
    "\n",
    "    #             if \"</summary>\" in line:\n",
    "    #                 in_line = false\n",
    "    #                 echo line.replace(\"</summary>\",\"\\n\\n\")\n",
    "    #             elif \"</title>\" in line:\n",
    "    #                 in_line = false\n",
    "    #                 echo line.replace(\"</title>\",\"\\n\")\n",
    "    #             else:\n",
    "    #                 echo line\n",
    "    # finally:\n",
    "    #     f.close()\n",
    "\n",
    "\n",
    "if isMainModule:\n",
    "\n",
    "    # Define consts and vars.\n",
    "    # 20,000 and 10,000 means the requests will randomly vary in milliseconds between at least 20s and 20+10=30s\n",
    "    const minRandDelayInMilliseconds = 20000\n",
    "    const additionalDelayInMilliseconds = 10000\n",
    "    var urlList: seq[string]\n",
    "\n",
    "    # For debugging.\n",
    "    var randDelay: int\n",
    "\n",
    "\n",
    "    # Process the urlList from urlFile.txt\n",
    "    try:\n",
    "        for line in lines \"/home/wrkn/GitRepos/HEPgpt/data-gathering/urlFile.txt\":\n",
    "            urlList.add(line)\n",
    "    except Exception:\n",
    "        echo getCurrentExceptionMsg()\n",
    "\n",
    "\n",
    "    # Put some space between the compiler line and first output line\n",
    "    echo \"\"\n",
    "\n",
    "    # Loop through the urls and return the request in files based on the id of the url.\n",
    "    # Also, implement a random request delay timer so we wait a minimum and max number of milliseconds between requests according to the robots.txt file (cern.atlas=20000ms min)\n",
    "    for idx, url in urlList:\n",
    "        \n",
    "        try:\n",
    "            return_request(url, idx)\n",
    "            echo &\"Request {idx} completed.\\n\"\n",
    "        except:\n",
    "            echo \"Request failed with error:\"\n",
    "            echo getCurrentExceptionMsg()\n",
    "        \n",
    "        # Do not execute sleep on the final iteration\n",
    "        if idx != len(urlList)-1:\n",
    "            # For debugging\n",
    "            randDelay = rand(additionalDelayInMilliseconds) + minRandDelayInMilliseconds\n",
    "            echo &\"randDelay={randDelay}ms\"\n",
    "            os.sleep(randDelay)\n",
    "\n",
    "            # Non debugging\n",
    "            # os.sleep(rand(additionalDelayInMilliseconds) + minRandDelayInMilliseconds)\n",
    "\n",
    "    echo \"All procesing complete.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('hepgpt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa4330bd32747e48d18ddc83e8d837a0e791710c7b2f05da0b57ce6034a1e1c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
